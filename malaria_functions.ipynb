{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "import os\n",
    "import math\n",
    "import openbabel\n",
    "import pybel\n",
    "import itertools\n",
    "from scipy.stats import expon\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem, MACCSkeys, rdMolDescriptors\n",
    "from rdkit.ML.Descriptors.MoleculeDescriptors import MolecularDescriptorCalculator\n",
    "from rdkit.Chem.AtomPairs import Pairs, Torsions\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "from rdkit.Chem.ChemicalFeatures import BuildFeatureFactory\n",
    "from collections import OrderedDict, Counter\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split, ShuffleSplit, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from ggplot import *\n",
    "from time import time\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 40, 40\n",
    "\n",
    "def reading_csv():\n",
    "    csv_file = pd.read_csv(\"lista.csv\", index_col=None, decimal=\",\", na_values=\"ND\")\n",
    "    return csv_file\n",
    "\n",
    "\n",
    "def data_preproc(df):\n",
    "    df_preproc = df.ix[:, ['ID', 'SMILES', 'ION_ACTIVITY', 'TEST_SET', 'PvsP']]\n",
    "    # Dropping every row with a NaN value in any of the columns (both PvsP and IRON_ACTIVITY)\n",
    "    df_preproc = df_preproc.dropna(subset=['ID', 'SMILES', 'ION_ACTIVITY'])\n",
    "    df_preproc.loc[:, 'ION_ACTIVITY'] = pd.to_numeric(df_preproc.loc[:, 'ION_ACTIVITY'], errors='coerce')\n",
    "    # Transforming PvsP (Potency vs Parasite (uMol) to log10(x)\n",
    "    df_preproc['pEC50'] = np.log10(df_preproc['PvsP'])\n",
    "\n",
    "    print (\"Number of instances of each level of ION_ACTIVITY\", \"\\n\", df_preproc['ION_ACTIVITY'].value_counts())\n",
    "\n",
    "    return df_preproc\n",
    "\n",
    "\n",
    "def series4_cmp(df):\n",
    "    id_series4 = pd.read_csv(\"osm_series4.csv\", index_col=None)\n",
    "    osm_series4 = pd.merge(df, id_series4, how='inner', on=['SMILES'])\n",
    "\n",
    "    print(\"Number of instances of each level of ION_ACTIVITY\", \"\\n\", osm_series4['ION_ACTIVITY'].value_counts())\n",
    "\n",
    "    return osm_series4\n",
    "\n",
    "\n",
    "def dragon_cmp(df):\n",
    "    norm_dragon = pd.read_csv(\"kellerberrin/NormDragon.csv\", index_col=None)\n",
    "    norm_dragon = pd.merge(df, norm_dragon, how='inner', on='SMILES')\n",
    "    return norm_dragon\n",
    "\n",
    "\n",
    "def smiles_to_rdkit(smiles):\n",
    "    mols = []\n",
    "    mols = [Chem.MolFromSmiles(x) for x in smiles]\n",
    "    return mols\n",
    "\n",
    "def smiles_to_pybel(smiles):\n",
    "    mols = []\n",
    "    mols = [pybel.readstring(\"smi\",x) for x in smiles]\n",
    "    return mols\n",
    "\n",
    "# Getting RDKit descriptors\n",
    "def rdkit_desc(df):\n",
    "    descs = [x[0] for x in Chem.Descriptors.descList]\n",
    "    two = ['SMILES', 'ION_ACTIVITY', 'TEST_SET']\n",
    "    col_r = two + descs\n",
    "    rddf = pd.DataFrame(columns=col_r, index=df.index.values)\n",
    "    rddf = rddf.fillna(0)\n",
    "    rddf.loc[:, (\"SMILES\", \"ION_ACTIVITY\", \"TEST_SET\")] = df.loc[:, (\"SMILES\", \"ION_ACTIVITY\", \"TEST_SET\")]\n",
    "    mrdk_list = smiles_to_rdkit(rddf.loc[:, \"SMILES\"])\n",
    "    rddf = rddf.assign(MOLECULES=mrdk_list)\n",
    "    list_descriptors = []\n",
    "    mdc = MolecularDescriptorCalculator(descs)\n",
    "    for index, row in rddf.iterrows():\n",
    "        results = mdc.CalcDescriptors(row[\"MOLECULES\"])\n",
    "        descriptors = dict(zip(descs, results))\n",
    "        descriptors.update(\n",
    "            {'SMILES': row[\"SMILES\"], 'ION_ACTIVITY': row[\"ION_ACTIVITY\"], 'MOLECULES': row[\"MOLECULES\"], 'TEST_SET': row[\"TEST_SET\"]})\n",
    "        list_descriptors.append(descriptors)\n",
    "\n",
    "    rdkit_df = pd.DataFrame(list_descriptors)\n",
    "    return rdkit_df\n",
    "\n",
    "#Getting Pybel descriptors\n",
    "def pybel_desc(df):\n",
    "    descs = pybel.descs\n",
    "    two = ['SMILES', 'ION_ACTIVITY']\n",
    "    col_r = two + descs\n",
    "    pydf = pd.DataFrame(columns=col_r, index=df.index.values)\n",
    "    pydf = pydf.fillna(0)\n",
    "    pydf.loc[:, (\"SMILES\", \"ION_ACTIVITY\")] = df.loc[:, (\"SMILES\", \"ION_ACTIVITY\")]\n",
    "    mpy_list = smiles_to_pybel(pydf.loc[:, \"SMILES\"])\n",
    "    pydf = pydf.assign(MOLECULES=mpy_list)\n",
    "    list_descriptors = []\n",
    "    for index, row in pydf.iterrows():\n",
    "        results = row[\"MOLECULES\"].calcdesc()\n",
    "        results.update(\n",
    "            {'SMILES': row[\"SMILES\"], 'ION_ACTIVITY': row[\"ION_ACTIVITY\"], 'MOLECULES': row[\"MOLECULES\"]})\n",
    "        list_descriptors.append(results)\n",
    "\n",
    "    pybel_df = pd.DataFrame(list_descriptors)\n",
    "    pybel_df = pybel_df.dropna(axis=1, how = 'all')\n",
    "    pybel_df = pybel_df.drop(['MOLECULES'], axis=1)\n",
    "    return pybel_df\n",
    "    \n",
    "\n",
    "def sanitize_smiles(df):\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            result = Chem.SanitizeMol(row[\"MOLECULES\"])\n",
    "            if result != Chem.SanitizeFlags.SANITIZE_NONE:\n",
    "                sanitized_smile = Chem.MolToSmiles(row[\"MOLECULES\"])\n",
    "                print(\"Sanitized SMILE %s, Compound ID:%s\", sanitized_smile)\n",
    "                df.set_value(index, \"SMILES\", sanitized_smile)\n",
    "        except:\n",
    "            print(\"Unable to Sanitize SMILE %s, Compound ID:%s\", row[\"SMILES\"])\n",
    "            print(\"Record Deleted.\")\n",
    "            df.drop(index, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Fingerprint descriptor calculator. Rdkit default values\n",
    "#def fingerprint_desc(df):\n",
    "#    fp_descs = ['MORGAN1024', 'MORGAN2048_1', 'MORGAN2048_2', 'MORGAN2048_3', 'MORGAN2048_4', 'MORGAN2048_5',\n",
    "#                'MORGAN2048_6', 'TOPOLOGICAL2048', 'MACCFP']\n",
    "#    two = ['SMILES', 'ION_ACTIVITY']\n",
    "#    col_f = two + fp_descs\n",
    "#    fpd = pd.DataFrame(columns=col_f, index=df.index.values)\n",
    "#    fpd = fpd.fillna(0)\n",
    "#    fpd.loc[:, (\"SMILES\", \"ION_ACTIVITY\")] = df.loc[:, (\"SMILES\", \"ION_ACTIVITY\")]\n",
    "#    mrdk_list = smiles_to_rdkit(fpd.loc[:, \"SMILES\"])\n",
    "#    fpd = fpd.assign(MOLECULES=mrdk_list)\n",
    "#    list_fpdescriptors = []\n",
    "#    for index, row in fpd.iterrows():\n",
    "#        morgan_1024 = AllChem.GetMorganFingerprintAsBitVect(row[\"MOLECULES\"], 4, nBits=1024)\n",
    "#        morgan_2048_1 = AllChem.GetMorganFingerprintAsBitVect(row[\"MOLECULES\"], 1, nBits=2048)\n",
    "#        morgan_2048_2 = AllChem.GetMorganFingerprintAsBitVect(row[\"MOLECULES\"], 2, nBits=2048)\n",
    "#        morgan_2048_3 = AllChem.GetMorganFingerprintAsBitVect(row[\"MOLECULES\"], 3, nBits=2048)\n",
    "#        morgan_2048_4 = AllChem.GetMorganFingerprintAsBitVect(row[\"MOLECULES\"], 4, nBits=2048)\n",
    "#        morgan_2048_5 = AllChem.GetMorganFingerprintAsBitVect(row[\"MOLECULES\"], 5, nBits=2048)\n",
    "#        morgan_2048_6 = AllChem.GetMorganFingerprintAsBitVect(row[\"MOLECULES\"], 6, nBits=2048)\n",
    "#        topological_2048 = AllChem.GetHashedTopologicalTorsionFingerprintAsBitVect(row[\"MOLECULES\"])\n",
    "#        macc = AllChem.GetMACCSKeysFingerprint(row[\"MOLECULES\"])\n",
    "#        fp_descriptors = {'SMILES': row[\"SMILES\"], 'ION_ACTIVITY': row[\"ION_ACTIVITY\"], 'MOLECULES': row[\"MOLECULES\"],\n",
    "#                          'MORGAN1024': morgan_1024, 'MORGAN2048_1': morgan_2048_1, 'MORGAN2048_2': morgan_2048_2,\n",
    "#                          'MORGAN2048_3': morgan_2048_3, 'MORGAN2048_4': morgan_2048_4, 'MORGAN2048_5': morgan_2048_5,\n",
    "#                          'MORGAN2048_6': morgan_2048_6, 'TOPOLOGICAL2048': topological_2048, 'MACCFP': macc}\n",
    "#        list_fpdescriptors.append(fp_descriptors)\n",
    "#    fp_df = pd.DataFrame(list_fpdescriptors)\n",
    "#    return fp_df\n",
    "\n",
    "\n",
    "def merge_dfs(df1, df2):\n",
    "    df_merge = pd.merge(df1, df2, how='inner', on=('SMILES', 'ION_ACTIVITY'))\n",
    "    # Checking duplicated rows\n",
    "    df_merge = df_merge.drop_duplicates(subset='SMILES')\n",
    "\n",
    "    return df_merge\n",
    "\n",
    "\n",
    "def df_to_ndarrays(df):\n",
    "    df.select_dtypes(include=['object'])\n",
    "\n",
    "    # Managing data types\n",
    "    #if(df['RDF125v'].dtype == 'object'):\n",
    "    #    df['RDF125v'] = df['RDF125v'].str.replace(',', '.')\n",
    "    #    df['RDF125v'] = pd.to_numeric(df['RDF125v'])\n",
    "    #if(df['RDF100p'].dtype == 'object'):\n",
    "    #    df['RDF100p'] = pd.to_numeric(df['RDF100p'])\n",
    "    #if (df['RDF140p'].dtype == 'object'):\n",
    "    #    df['RDF140p'] = pd.to_numeric(df['RDF140p'])\n",
    "    # Removing compounds with activity = 0.5 (not statistically valuable)\n",
    "    df = df.drop(df[df.ION_ACTIVITY == 0.5].index)\n",
    "    #df = df.drop(['SMILES', 'ID', 'TEST_SET', 'pEC50', 'PvsP', 'MOLECULES'], axis=1)\n",
    "    #df = df.drop(['SMILES', 'TEST_SET', 'MOLECULES'], axis=1)\n",
    "    \n",
    "    df = df.drop(['SMILES', 'MOLECULES'], axis=1)\n",
    "    #df = df.drop('SMILES', axis=1)\n",
    "    \n",
    "    # Removing non-numeric variables\n",
    "    npdesc = df.drop(['ION_ACTIVITY'], axis=1).values\n",
    "    labdesc = df.ix[:, \"ION_ACTIVITY\"].values\n",
    "    # Scaling data\n",
    "    #npdesc = preprocessing.scale(npdesc)\n",
    "    return npdesc, labdesc\n",
    "\n",
    "\n",
    "def splitting_as_competition(df, fp=False):\n",
    "    df.select_dtypes(include=['object'])\n",
    "        \n",
    "    df = df.drop(df[df.ION_ACTIVITY == 0.5].index)\n",
    "        \n",
    "    osm_training = df.ix[(df['TEST_SET'] != 'B') & (df['TEST_SET'] != 'C'), :]\n",
    "    osm_testing = df.ix[(df['TEST_SET'] == 'B') | (df['TEST_SET'] == 'C'), :]\n",
    "        \n",
    "    osm_training = osm_training.drop(['SMILES', 'TEST_SET', 'MOLECULES'], axis=1)\n",
    "    osm_testing = osm_testing.drop(['SMILES', 'TEST_SET', 'MOLECULES'], axis=1)\n",
    "    \n",
    "    if fp == True:\n",
    "        osm_training_fp = osm_training.ix[:,fp_names]\n",
    "        osm_testing_fp = osm_testing.ix[:,fp_names]\n",
    "        \n",
    "        osm_training = osm_training.drop(fp_names, axis=1)\n",
    "        osm_testing = osm_testing.drop(fp_names, axis=1)\n",
    "        \n",
    "        nptrain_fp = osm_training_fp.as_matrix()\n",
    "        nptest_fp = osm_testing_fp.as_matrix()\n",
    "    \n",
    "    nptrain = osm_training.drop(['ION_ACTIVITY'], axis=1).as_matrix()\n",
    "    labtrain = osm_training.ix[:, \"ION_ACTIVITY\"].as_matrix()\n",
    "    nptest = osm_testing.drop(['ION_ACTIVITY'], axis=1).as_matrix()\n",
    "    labtest = osm_testing.ix[:, \"ION_ACTIVITY\"].as_matrix()\n",
    "    \n",
    "    # Scaling data\n",
    "    nptrain = preprocessing.scale(nptrain)\n",
    "    nptest = preprocessing.scale(nptest)\n",
    "    \n",
    "    if fp == True:\n",
    "        nptrain = np.concatenate((nptrain, nptrain_fp), axis=1)\n",
    "        nptest = np.concatenate((nptest, nptest_fp), axis=1)\n",
    "    \n",
    "    return nptrain, nptest, labtrain, labtest\n",
    "    \n",
    "\n",
    "# def splitting_as_competition(df):\n",
    "#     df.select_dtypes(include=['object'])\n",
    "\n",
    "#     #if(df['RDF125v'].dtype == 'object'):\n",
    "#     #    df['RDF125v'] = df['RDF125v'].str.replace(',', '.')\n",
    "#     #    df['RDF125v'] = pd.to_numeric(df['RDF125v'])\n",
    "#     #if(df['RDF100p'].dtype == 'object'):\n",
    "#     #    df['RDF100p'] = pd.to_numeric(df['RDF100p'])\n",
    "#     #if (df['RDF140p'].dtype == 'object'):\n",
    "#     #    df['RDF140p'] = pd.to_numeric(df['RDF140p'])\n",
    "        \n",
    "#     df = df.drop(df[df.ION_ACTIVITY == 0.5].index)\n",
    "\n",
    "#     osm_training = df.ix[(df['TEST_SET'] != 'B') & (df['TEST_SET'] != 'C'), :]\n",
    "#     osm_testing = df.ix[(df['TEST_SET'] == 'B') | (df['TEST_SET'] == 'C'), :]\n",
    "\n",
    "\n",
    "#     osm_training = osm_training.drop(['SMILES', 'TEST_SET', 'MOLECULES'], axis=1)\n",
    "#     osm_testing = osm_testing.drop(['SMILES', 'TEST_SET', 'MOLECULES'], axis=1)\n",
    "    \n",
    "#     #########################################################################\n",
    "#     #included 06/06\n",
    "#     osm_training_fp = osm_training.ix[:,fp_names]\n",
    "#     osm_testing_fp = osm_testing.ix[:,fp_names]\n",
    "    \n",
    "#     osm_training = osm_training.drop(fp_names, axis=1)\n",
    "#     osm_testing = osm_testing.drop(fp_names, axis=1)\n",
    "    \n",
    "#     #########################################################################\n",
    "#     nptrain = osm_training.drop(['ION_ACTIVITY'], axis=1).as_matrix()\n",
    "#     labtrain = osm_training.ix[:, \"ION_ACTIVITY\"].as_matrix()\n",
    "#     nptest = osm_testing.drop(['ION_ACTIVITY'], axis=1).as_matrix()\n",
    "#     labtest = osm_testing.ix[:, \"ION_ACTIVITY\"].as_matrix()\n",
    "    \n",
    "#     #########################################################################\n",
    "#     #included 06/06\n",
    "#     nptrain_fp = osm_training_fp.as_matrix()\n",
    "#     nptest_fp = osm_testing_fp.as_matrix()\n",
    "#     #########################################################################\n",
    "    \n",
    "#     # Scaling data\n",
    "#     nptrain = preprocessing.scale(nptrain)\n",
    "#     nptest = preprocessing.scale(nptest)\n",
    "    \n",
    "#     #########################################################################\n",
    "#     #included 06/06\n",
    "#     nptrain = np.concatenate((nptrain, nptrain_fp), axis=1)\n",
    "#     nptest = np.concatenate((nptest, nptest_fp), axis=1)\n",
    "#     #########################################################################\n",
    "#     return nptrain, nptest, labtrain, labtest\n",
    "\n",
    "def balancing_classes(npdesc, labdesc, method):\n",
    "    print(\"Original dataset shape {}\".format(Counter(labdesc)))\n",
    "    # Goal: balancing the dataset\n",
    "    if method == \"smote\":\n",
    "        mod = SMOTE(random_state=30)\n",
    "    elif method == \"rov\":\n",
    "        mod = RandomOverSampler(random_state=30)\n",
    "    elif method == \"adasyn\":\n",
    "        mod = ADASYN(random_state=30)\n",
    "    else:\n",
    "        print(\"The oversampling method introduced is not available\")\n",
    "\n",
    "    npdesc_res, labdesc_res = mod.fit_sample(npdesc, labdesc)\n",
    "    print('Resampled dataset shape {}'.format(Counter(labdesc_res)))\n",
    "    return npdesc_res, labdesc_res\n",
    "\n",
    "\n",
    "def feature_sel(npdesc_original, npdesc_bal, labdesc_original, esti):\n",
    "    if esti == \"svm_lin\":\n",
    "        estimator = SVC(kernel=\"linear\")\n",
    "    elif esti == \"log\":\n",
    "        estimator = LogisticRegression()\n",
    "    else:\n",
    "        print(\"The estimation method introduced is not available\")\n",
    "    # Filtering half of the features\n",
    "    selector = RFECV(estimator, cv=5,)\n",
    "    # Using original data to perform feature selection\n",
    "    npdesc_filt = selector.fit(npdesc_original, labdesc_original)\n",
    "    print(\"Optimal number of features: %d\" % npdesc_filt.n_features_)\n",
    "\n",
    "    # Filtering selected features from the data array\n",
    "    npdesc_fs = npdesc_bal[:, npdesc_filt.support_]\n",
    "    return npdesc_fs\n",
    "\n",
    "def splitting_data(npdesc,labdesc, test_size):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(npdesc, labdesc, test_size=test_size, random_state=0)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def classification(x_train, x_test, y_train, y_test, modelo):\n",
    "    # splitting data in 80% training set, 20% test set\n",
    "    # x_train, x_test, y_train, y_test = train_test_split(npdesc, labdesc, test_size=0.3, random_state=0)\n",
    "    # Logistic regression\n",
    "    if modelo == \"log_res\":\n",
    "        params = {\"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "        #model = LogisticRegressionCV(penalty='l2', cv=10)\n",
    "        ran_search = GridSearchCV(estimator=LogisticRegression(penalty='l2',intercept_scaling=1, dual=False, \n",
    "                                                               fit_intercept=True, tol=0.0001), \n",
    "                                  param_grid=params, cv=10)\n",
    "        ran_search = ran_search.fit(x_train, y_train)\n",
    "        model = ran_search.best_estimator_\n",
    "        print(\"Best parameters set found on development set:\", ran_search.best_params_)\n",
    "\n",
    "    # SVC\n",
    "    elif modelo == \"svm\":\n",
    "        # params = [{'C': stats.expon(scale=10), 'gamma': stats.expon(scale=0.1), 'kernel': ['linear']},\n",
    "        #          {'C': stats.expon(scale=10), 'gamma': stats.expon(scale=0.1), 'kernel': ['rbf']},\n",
    "        #          {'C': stats.expon(scale=10), 'gamma': stats.expon(scale=0.1), 'kernel': ['sigmoid']},\n",
    "        #          {'C': stats.expon(scale=10), 'gamma': stats.expon(scale=0.1), 'degree': [2, 3], 'kernel': ['poly']}]\n",
    "        # params = {'C': expon(scale=10), 'gamma': expon(scale=0.1), 'kernel': [str('linear'), str('rbf'), str('sigmoid'), str('poly')], 'degree':[2, 3]}\n",
    "        # Randomized search\n",
    "        params = {'C': [1e-3, 1e-2, 1e-1, 1, 5, 10, 100], 'gamma': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000],\n",
    "                  'kernel': [str('linear'), str('rbf'), str('sigmoid'), str('poly')], 'degree': [2, 3]}\n",
    "        #ran_search = RandomizedSearchCV(estimator=SVC(probability=True), param_distributions=params, cv=10, n_iter=100, n_jobs=1)\n",
    "        ran_search = GridSearchCV(estimator=SVC(probability=True), param_grid=params, cv=10)\n",
    "        start = time()\n",
    "        ran_search = ran_search.fit(x_train, y_train)\n",
    "        print(\"RandomizedSearchCV took %.2f seconds\" % (time() - start))\n",
    "        model = ran_search.best_estimator_\n",
    "        print(\"Best parameters set found on development set:\", ran_search.best_params_)\n",
    "        print(\"Grid scores on development set:\")\n",
    "        means = ran_search.cv_results_['mean_test_score']\n",
    "        stds = ran_search.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, ran_search.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    # Random Forests\n",
    "    elif modelo == \"rforest\":\n",
    "        # Designate distributions to sample hyperparameters from\n",
    "        #n_estimators = np.random.uniform(70, 80, 5).astype(int)\n",
    "        #max_features = np.random.normal(6, 3, 5).astype(int)\n",
    "        # Check max_features>0 & max_features<=total number of features\n",
    "        #max_features[max_features <= 0] = 1\n",
    "        #max_features[max_features > x_train.shape[1]] = x_train.shape[1]\n",
    "        # hyperparameters = {'n_estimators': list(n_estimators),\n",
    "        #                   'max_features': list(max_features)}\n",
    "        hyperparameters = {'n_estimators': [10, 50, 100, 150, 200, 250, 300], 'max_depth': sp_randint(1,x_train.shape[1]),\n",
    "                           'max_features': sp_randint(1,x_train.shape[1]), 'criterion': [str('gini'), str('entropy')],\n",
    "                           'bootstrap':[True, False], \"min_samples_split\": sp_randint(2, x_train.shape[1]),\n",
    "                           'min_samples_leaf': [1, 10, 20, 30, 40, 50]}\n",
    "        ran_search = RandomizedSearchCV(RandomForestClassifier(), param_distributions=hyperparameters, n_iter=100, n_jobs=1, cv=10)\n",
    "        ran_search = ran_search.fit(x_train, y_train)\n",
    "        model = ran_search.best_estimator_\n",
    "        print(\"Best parameters set found on development set:\", ran_search.best_params_)\n",
    "\n",
    "    # Nearest Neighbours\n",
    "    elif modelo == \"knn\":\n",
    "        params = {\"n_neighbors\": [1, 3, 5, 7, 9], \"metric\": [str(\"euclidean\"), str(\"manhattan\"), str(\"chebyshev\"), str(\"minkowski\")], \"p\":[2]}\n",
    "        ran_search = GridSearchCV(KNeighborsClassifier(algorithm='auto', p=2), param_grid=params)\n",
    "        ran_search = ran_search.fit(x_train, y_train)\n",
    "        model = ran_search.best_estimator_\n",
    "        print(\"Best parameters set found on development set:\", ran_search.best_params_)\n",
    "\n",
    "    else:\n",
    "        print(\"The prediction model introduced is not available\")\n",
    "\n",
    "    model = model.fit(x_train, y_train)\n",
    "\n",
    "    probs = model.predict_proba(x_test)\n",
    "\n",
    "    # if modelo == \"log_res\":\n",
    "    #     print(pd.DataFrame(zip(x_test.columns, np.transpose(modelo.coef_))))\n",
    "    \n",
    "    print(\"MODEL USED: \", modelo)\n",
    "\n",
    "    # Accuracy on the training set\n",
    "    print('Accuracy on the training set:', model.score(x_train, y_train))\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    print (\"The accuracy predicting in the test set is \", metrics.accuracy_score(y_test, y_pred))\n",
    "    print (\"The AUC of the ROC curve is\", metrics.roc_auc_score(y_test, probs[:, 1]))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print (metrics.confusion_matrix(y_test, y_pred))\n",
    "    print(\"Detailed classification report:\")\n",
    "    print (metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "    # Computing ROC curve\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, probs[:, 1])\n",
    "\n",
    "    df = pd.DataFrame(dict(fpr=fpr, tpr=tpr))\n",
    "    x = ggplot(df, aes(x='fpr', y='tpr')) + \\\n",
    "        geom_line() + \\\n",
    "        geom_abline(linetype='dashed') + \\\n",
    "        scale_x_continuous(limits=(0,1)) + \\\n",
    "        scale_y_continuous(limits=(0,1)) + \\\n",
    "        xlab('FPR') + \\\n",
    "        ylab('TPR') + \\\n",
    "        ggtitle(\"ROC Curve w/ AUC=%s\" % str(metrics.auc(fpr,tpr)))\n",
    "    return x\n",
    "    \n",
    "    \n",
    "def xgboost_prueba(alg, x_train, x_test, y_train, y_test, cv_folds=5, early_stopping_rounds=50):\n",
    "    # loading numpy array into dMatrix\n",
    "    xgb_train = xgb.DMatrix( x_train, label=y_train)\n",
    "    xgb_test = xgb.DMatrix( x_test, label=y_test)\n",
    "    \n",
    "    #Train cross validation\n",
    "    xgb_param = alg.get_xgb_params()\n",
    "    \n",
    "    cvresult = xgb.cv(xgb_param, xgb_train, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "                      metrics = ['logloss'], early_stopping_rounds=early_stopping_rounds)\n",
    "    alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(x_train, y_train, eval_metric='auc')\n",
    "    #alg.fit(x_train, y_train, eval_metric='logloss')\n",
    "    \n",
    "    #Predict in training set\n",
    "    y_trpred = alg.predict(x_train)\n",
    "    y_trpredprob = alg.predict_proba(x_train)[:,1]\n",
    "    \n",
    "    #Predict in test set\n",
    "    y_pred = alg.predict(x_test)\n",
    "    y_predprob = alg.predict_proba(x_test)[:,1]\n",
    "    \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    \n",
    "    print (\"Accuracy (train set): %.4g\" % metrics.accuracy_score(y_train, y_trpred))\n",
    "    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(y_train, y_trpredprob))\n",
    "    \n",
    "    print (\"Accuracy (test set): %.4g\" % metrics.accuracy_score(y_test, y_pred))\n",
    "    print (\"AUC Score (Test): %f\" % metrics.roc_auc_score(y_test, y_predprob))    \n",
    "    \n",
    "    #parte a√±adida que puede petar\n",
    "    ################################################################################\n",
    "    print(\"Confusion matrix:\")\n",
    "    print (metrics.confusion_matrix(y_test, y_pred))\n",
    "    print(\"Detailed classification report:\")\n",
    "    print (metrics.classification_report(y_test, y_pred))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')\n",
    "    \n",
    "        # Computing ROC curve\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_predprob)\n",
    "\n",
    "    df = pd.DataFrame(dict(fpr=fpr, tpr=tpr))\n",
    "    x = ggplot(df, aes(x='fpr', y='tpr')) + \\\n",
    "        geom_line() + \\\n",
    "        geom_abline(linetype='dashed') + \\\n",
    "        scale_x_continuous(limits=(0,1)) + \\\n",
    "        scale_y_continuous(limits=(0,1)) + \\\n",
    "        xlab('FPR') + \\\n",
    "        ylab('TPR') + \\\n",
    "        ggtitle(\"ROC Curve w/ AUC=%s\" % str(metrics.auc(fpr,tpr)))\n",
    "    return x\n",
    "    \n",
    "\n",
    "    \n",
    "def xgboost_tuning(x_train, y_train, rat):\n",
    "    #cr_va = 3\n",
    "    random.seed(6)\n",
    "    cr_va = StratifiedKFold(n_splits=3, shuffle=False, random_state=1)\n",
    "    \n",
    "    #Add feature selection part\n",
    "    \n",
    "    #Define \"baseline\" model\n",
    "    #xgbl = XGBClassifier(learning_rate =0.1, n_estimators=1000, max_depth=5, min_child_weight=1,gamma=0, subsample=0.8, \n",
    "    #                     colsample_bytree=0.8, objective= 'binary:logistic', scale_pos_weight=1, seed=27)\n",
    "    \n",
    "    #Tuning max_depth and min_child_weight\n",
    "    param_test1 = {'max_depth':range(3,10,2), 'min_child_weight':range(1,6,2)}\n",
    "\n",
    "    gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5, \n",
    "                                                      min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, \n",
    "                                                      #objective= 'binary:logistic', scale_pos_weight=1, seed=27),\n",
    "                                                      objective= 'binary:logistic', scale_pos_weight=rat),\n",
    "                           # param_grid = param_test1, scoring='roc_auc',iid=False, cv=5)\n",
    "                            param_grid = param_test1, scoring='roc_auc',iid=False, cv=cr_va)\n",
    "    \n",
    "    gsearch1.fit(x_train,y_train)\n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    max_depth = gsearch1.best_params_['max_depth']\n",
    "    min_child_weight= gsearch1.best_params_['min_child_weight'] \n",
    "    \n",
    "    #Tuning gamma\n",
    "    param_test2 = {'gamma':[i/10.0 for i in range(0,5)]}\n",
    "    gsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=max_depth, \n",
    "                                                      min_child_weight= min_child_weight, gamma=0, subsample=0.8, \n",
    "                                                      colsample_bytree=0.8, objective= 'binary:logistic', \n",
    "                                                      #scale_pos_weight=1,seed=27), \n",
    "                                                      scale_pos_weight=rat), \n",
    "                           # param_grid = param_test2, scoring='roc_auc',iid=False, cv=5)\n",
    "                            param_grid = param_test2, scoring='roc_auc',iid=False, cv=cr_va)\n",
    "                            \n",
    "    gsearch2.fit(x_train,y_train)\n",
    "    print(gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_)\n",
    "    gamma= gsearch2.best_params_['gamma']\n",
    "    \n",
    "    #Tuning subsample and colsample_bytree\n",
    "    param_test3 = {'subsample':[i/10.0 for i in range(6,10)], 'colsample_bytree':[i/10.0 for i in range(6,10)]}\n",
    "    gsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=max_depth,\n",
    "                                                      min_child_weight=min_child_weight, gamma=gamma, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                      #objective= 'binary:logistic', scale_pos_weight=1,seed=27), \n",
    "                                                      objective= 'binary:logistic', scale_pos_weight=rat), \n",
    "                            # param_grid = param_test3, scoring='roc_auc', iid=False, cv=5)\n",
    "                            param_grid = param_test3, scoring='roc_auc', iid=False, cv=cr_va)\n",
    "    gsearch3.fit(x_train, y_train)\n",
    "    print(gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_)\n",
    "    subsample = gsearch3.best_params_['subsample']\n",
    "    colsample_bytree = gsearch3.best_params_['colsample_bytree']\n",
    "    \n",
    "    #Retuning subsample and colsample_bytree\n",
    "    param_test3b = {'subsample':[i/100.0 for i in range(int(subsample*100)-5,95,5)], \n",
    "                    'colsample_bytree':[i/100.0 for i in range(int(colsample_bytree*100)-5,95,5)]}\n",
    "    gsearch3b = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=max_depth,\n",
    "                                                       min_child_weight=min_child_weight, gamma=gamma, subsample=subsample, \n",
    "                                                       colsample_bytree=colsample_bytree,\n",
    "                                                      #objective= 'binary:logistic', scale_pos_weight=1,seed=27), \n",
    "                                                       objective= 'binary:logistic', scale_pos_weight=rat),\n",
    "                            # param_grid = param_test3, scoring='roc_auc', iid=False, cv=5)\n",
    "                            param_grid = param_test3b, scoring='roc_auc', iid=False, cv=cr_va)\n",
    "    gsearch3b.fit(x_train, y_train)\n",
    "    print(gsearch3b.grid_scores_, gsearch3b.best_params_, gsearch3b.best_score_)\n",
    "    subsample = gsearch3b.best_params_['subsample']\n",
    "    colsample_bytree = gsearch3b.best_params_['colsample_bytree']\n",
    "    \n",
    "    #Tuning reg_alpha \n",
    "    param_test4 = {'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]}\n",
    "\n",
    "    gsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=max_depth,\n",
    "                                                      min_child_weight=min_child_weight, gamma=gamma, subsample=subsample, \n",
    "                                                      colsample_bytree=colsample_bytree, objective= 'binary:logistic',  \n",
    "                                                      #scale_pos_weight=1,seed=27), \n",
    "                                                      scale_pos_weight=rat), \n",
    "                        # param_grid = param_test4, scoring='roc_auc', iid=False, cv=5)\n",
    "                            param_grid = param_test4, scoring='roc_auc', iid=False, cv=cr_va)\n",
    "    gsearch4.fit(x_train,y_train)\n",
    "    print(gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_)\n",
    "    reg_alpha = gsearch4.best_params_['reg_alpha']\n",
    "    \n",
    "    #Tuning learning_rate and n_estimators\n",
    "    param_test5= {'learning_rate' : [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3], 'n_estimators' : [100, 200, 300, 400, 500]}\n",
    "    gsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=max_depth,\n",
    "                                                      min_child_weight=min_child_weight, gamma=gamma, subsample=subsample, \n",
    "                                                      colsample_bytree=colsample_bytree, objective= 'binary:logistic',  \n",
    "                                                      #scale_pos_weight=1,seed=27),\n",
    "                                                     scale_pos_weight=rat),\n",
    "                        # param_grid = param_test5, scoring='roc_auc', iid=False, cv=5)\n",
    "                        param_grid = param_test5, scoring='roc_auc', iid=False, cv=cr_va)\n",
    "\n",
    "    gsearch5.fit(x_train,y_train)\n",
    "    print(gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_)\n",
    "    learning_rate = gsearch5.best_params_['learning_rate']\n",
    "    n_estimators = gsearch5.best_params_['n_estimators']\n",
    "    \n",
    "    optim_model = XGBClassifier(learning_rate = learning_rate ,n_estimators= n_estimators, max_depth=max_depth,\n",
    "                                min_child_weight=min_child_weight,gamma=gamma,subsample=subsample,\n",
    "                                colsample_bytree=colsample_bytree, reg_alpha=reg_alpha, objective= 'binary:logistic',\n",
    "                                #scale_pos_weight=1,seed=27)\n",
    "                                scale_pos_weight=rat)\n",
    "    \n",
    "    return optim_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "#Fingerprint calculator\n",
    "nbits = 1024\n",
    "longbits = 16384\n",
    "\n",
    "# dictionary\n",
    "fpdict = {}\n",
    "#fpdict['ecfp0'] = lambda m: AllChem.GetMorganFingerprintAsBitVect(m, 0, nBits=nbits)\n",
    "#fpdict['ecfp2'] = lambda m: AllChem.GetMorganFingerprintAsBitVect(m, 1, nBits=nbits)\n",
    "#fpdict['ecfp4'] = lambda m: AllChem.GetMorganFingerprintAsBitVect(m, 2, nBits=nbits)\n",
    "fpdict['ecfp6'] = lambda m: AllChem.GetMorganFingerprintAsBitVect(m, 3, nBits=nbits)\n",
    "#fpdict['ecfc0'] = lambda m: AllChem.GetMorganFingerprint(m, 0)\n",
    "#fpdict['ecfc2'] = lambda m: AllChem.GetMorganFingerprint(m, 1)\n",
    "#fpdict['ecfc4'] = lambda m: AllChem.GetMorganFingerprint(m, 2)\n",
    "#fpdict['ecfc6'] = lambda m: AllChem.GetMorganFingerprint(m, 3)\n",
    "#fpdict['fcfp2'] = lambda m: AllChem.GetMorganFingerprintAsBitVect(m, 1, useFeatures=True, nBits=nbits)\n",
    "#fpdict['fcfp4'] = lambda m: AllChem.GetMorganFingerprintAsBitVect(m, 2, useFeatures=True, nBits=nbits)\n",
    "fpdict['fcfp6'] = lambda m: AllChem.GetMorganFingerprintAsBitVect(m, 3, useFeatures=True, nBits=nbits)\n",
    "#fpdict['fcfc2'] = lambda m: AllChem.GetMorganFingerprint(m, 1, useFeatures=True)\n",
    "#fpdict['fcfc4'] = lambda m: AllChem.GetMorganFingerprint(m, 2, useFeatures=True)\n",
    "#fpdict['fcfc6'] = lambda m: AllChem.GetMorganFingerprint(m, 3, useFeatures=True)\n",
    "#fpdict['lecfp4'] = lambda m: AllChem.GetMorganFingerprintAsBitVect(m, 2, nBits=longbits)\n",
    "#fpdict['lecfp6'] = lambda m: AllChem.GetMorganFingerprintAsBitVect(m, 3, nBits=longbits)\n",
    "#fpdict['lfcfp4'] = lambda m: AllChem.GetMorganFingerprintAsBitVect(m, 2, useFeatures=True, nBits=longbits)\n",
    "#fpdict['lfcfp6'] = lambda m: AllChem.GetMorganFingerprintAsBitVect(m, 3, useFeatures=True, nBits=longbits)\n",
    "\n",
    "#fpdict['maccs'] = lambda m: MACCSkeys.GenMACCSKeys(m)\n",
    "\n",
    "#fpdict['ap'] = lambda m: Pairs.GetAtomPairFingerprint(m)\n",
    "#fpdict['tt'] = lambda m: Torsions.GetTopologicalTorsionFingerprintAsIntVect(m)\n",
    "fpdict['hashap'] = lambda m: rdMolDescriptors.GetHashedAtomPairFingerprintAsBitVect(m, nBits=nbits)\n",
    "fpdict['hashtt'] = lambda m: rdMolDescriptors.GetHashedTopologicalTorsionFingerprintAsBitVect(m, nBits=nbits)\n",
    "#fpdict['avalon'] = lambda m: fpAvalon.GetAvalonFP(m, nbits)\n",
    "#fpdict['laval'] = lambda m: fpAvalon.GetAvalonFP(m, longbits)\n",
    "#fpdict['rdk5'] = lambda m: Chem.RDKFingerprint(m, maxPath=5, fpSize=nbits, nBitsPerHash=2)\n",
    "#fpdict['rdk6'] = lambda m: Chem.RDKFingerprint(m, maxPath=6, fpSize=nbits, nBitsPerHash=2)\n",
    "fpdict['rdk7'] = lambda m: Chem.RDKFingerprint(m, maxPath=7, fpSize=nbits, nBitsPerHash=2)\n",
    "\n",
    "fp_names = fpdict.keys()\n",
    "\n",
    "# Fingerprint descriptor calculator. Rdkit default values\n",
    "def fingerprint_desc(df):\n",
    "    #fp_descs = ['MORGAN1024', 'MORGAN2048_1', 'MORGAN2048_2', 'MORGAN2048_3', 'MORGAN2048_4', 'MORGAN2048_5',\n",
    "    #            'MORGAN2048_6', 'TOPOLOGICAL2048', 'MACCFP']\n",
    "    two = ['SMILES', 'ION_ACTIVITY']\n",
    "    col_f = two + fp_names\n",
    "    fpd = pd.DataFrame(columns=col_f, index=df.index.values)\n",
    "    fpd = fpd.fillna(0)\n",
    "    fpd.loc[:, (\"SMILES\", \"ION_ACTIVITY\")] = df.loc[:, (\"SMILES\", \"ION_ACTIVITY\")]\n",
    "    mrdk_list = smiles_to_rdkit(fpd.loc[:, \"SMILES\"])\n",
    "    fpd = fpd.assign(MOLECULES=mrdk_list)\n",
    "    list_fpdescriptors = []\n",
    "    for index, row in fpd.iterrows():\n",
    "        results = {}\n",
    "        for fp in fp_names:\n",
    "            results[fp] = fpdict[fp](row[\"MOLECULES\"])\n",
    "        np_fps = {}\n",
    "        for key, value in results.iteritems():\n",
    "            arr = np.zeros((1,))\n",
    "            if isinstance(value, DataStructs.cDataStructs.ExplicitBitVect):\n",
    "                DataStructs.ConvertToNumpyArray(value, arr)\n",
    "            else:\n",
    "                if isinstance(value, DataStructs.cDataStructs.UIntSparseIntVect):\n",
    "                    fp_bin = DataStructs.cDataStructs.UIntSparseIntVect.ToBinary(value)\n",
    "                elif isinstance(value, DataStructs.cDataStructs.IntSparseIntVect):\n",
    "                    fp_bin = DataStructs.cDataStructs.IntSparseIntVect.ToBinary(value)\n",
    "                else:\n",
    "                    fp_bin = DataStructs.cDataStructs.LongSparseIntVect.ToBinary(value)\n",
    "                fp_frombin = DataStructs.cDataStructs.CreateFromBinaryText(fp_bin)\n",
    "                DataStructs.ConvertToNumpyArray(fp_frombin, arr)\n",
    "            np_fps[key] = arr.tolist()\n",
    "        #morgan_1024 = AllChem.GetMorganFingerprintAsBitVect(row[\"MOLECULES\"], 4, nBits=1024)\n",
    "        #morgan_2048_1 = AllChem.GetMorganFingerprintAsBitVect(row[\"MOLECULES\"], 1, nBits=2048)\n",
    "        #morgan_2048_2 = AllChem.GetMorganFingerprintAsBitVect(row[\"MOLECULES\"], 2, nBits=2048)\n",
    "        #morgan_2048_3 = AllChem.GetMorganFingerprintAsBitVect(row[\"MOLECULES\"], 3, nBits=2048)\n",
    "        #morgan_2048_4 = AllChem.GetMorganFingerprintAsBitVect(row[\"MOLECULES\"], 4, nBits=2048)\n",
    "        #morgan_2048_5 = AllChem.GetMorganFingerprintAsBitVect(row[\"MOLECULES\"], 5, nBits=2048)\n",
    "        #morgan_2048_6 = AllChem.GetMorganFingerprintAsBitVect(row[\"MOLECULES\"], 6, nBits=2048)\n",
    "        #topological_2048 = AllChem.GetHashedTopologicalTorsionFingerprintAsBitVect(row[\"MOLECULES\"])\n",
    "        #macc = AllChem.GetMACCSKeysFingerprint(row[\"MOLECULES\"])\n",
    "            np_fps.update({'SMILES': row[\"SMILES\"], 'ION_ACTIVITY': row[\"ION_ACTIVITY\"], 'MOLECULES': row[\"MOLECULES\"]})\n",
    "        #fp_descriptors = {'SMILES': row[\"SMILES\"], 'ION_ACTIVITY': row[\"ION_ACTIVITY\"], 'MOLECULES': row[\"MOLECULES\"],\n",
    "        #                  'MORGAN1024': morgan_1024, 'MORGAN2048_1': morgan_2048_1, 'MORGAN2048_2': morgan_2048_2,\n",
    "        #                  'MORGAN2048_3': morgan_2048_3, 'MORGAN2048_4': morgan_2048_4, 'MORGAN2048_5': morgan_2048_5,\n",
    "        #                  'MORGAN2048_6': morgan_2048_6, 'TOPOLOGICAL2048': topological_2048, 'MACCFP': macc}\n",
    "        list_fpdescriptors.append(np_fps)\n",
    "    fp_df = pd.DataFrame(list_fpdescriptors)\n",
    "    \n",
    "    ################################################################\n",
    "    #inserted 06/06\n",
    "    for index, row in fp_df.iterrows():\n",
    "        for fp in fp_names:\n",
    "            ar_val = np.array(fp_df.ix[index, fp])\n",
    "            fp_df.set_value(index, fp, ar_val)\n",
    "            \n",
    "    fp_df = fp_df.drop(['MOLECULES'], axis=1)\n",
    "    \n",
    "    return fp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spectrophores(df):\n",
    "    \"\"\"\n",
    "    Spectrophores descriptor calculator.\n",
    "    \"\"\"\n",
    "    two = ['SMILES', 'ION_ACTIVITY']\n",
    "    sp_names = [j+str(i) for i,j in zip(range(1,49),[\"sp\"] * 48)]\n",
    "    #col_s = two + ['SPECTROPHORES'] + sp_names\n",
    "    col_s = two + sp_names\n",
    "    spd = pd.DataFrame(columns=col_s, index=df.index.values)\n",
    "    spd = spd.fillna(0)\n",
    "    spd.loc[:, (\"SMILES\", \"ION_ACTIVITY\")] = df.loc[:, (\"SMILES\", \"ION_ACTIVITY\")]\n",
    "    mpy_list = smiles_to_pybel(spd.loc[:, \"SMILES\"])\n",
    "    spd = spd.assign(MOLECULES=mpy_list)\n",
    "    #spd['SPECTROPHORES'] = spd['SPECTROPHORES'].astype(object)\n",
    "    #list_spdescriptors = []\n",
    "    \n",
    "    for index, row in spd.iterrows():\n",
    "        row[\"MOLECULES\"].make3D()\n",
    "        spectrophore = pybel.ob.OBSpectrophore()\n",
    "        spectrophore.SetNormalization(spectrophore.NormalizationTowardsZeroMeanAndUnitStd)\n",
    "        hey = spectrophore.GetSpectrophore(row[\"MOLECULES\"].OBMol)\n",
    "        sp_descriptors = {n: d for n, d in zip(sp_names, hey)}\n",
    "        \n",
    "        for key in sp_descriptors.keys():\n",
    "            spd.loc[index, key] = sp_descriptors[key]\n",
    "    \n",
    "    \n",
    "    spd = spd.dropna(axis=1, how='all')\n",
    "    return spd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fp_individual(df):  \n",
    "    fp_names_short = ['rdk7', 'rdk5', 'hashtt', 'ecfp6', 'ecfp4', 'ecfp2', 'ecfp0', 'fcfp2', 'fcfp4', 'rdk6', 'fcfp6', 'hashap']\n",
    "    fp_names_long = ['lfcfp6', 'lfcfp4', 'lecfp4', 'lecfp6']\n",
    "    fp_names_ss =  ['maccs']\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        for name in fp_names:\n",
    "            if name in fp_names_short:\n",
    "                name_fp = [j+\"_\"+str(i) for i, j in zip(range(1,nbits+1), [name]*(nbits+1))]\n",
    "            \n",
    "            elif name in fp_names_long:\n",
    "                name_fp = [j+\"_\"+str(i) for i, j in zip(range(1,longbits+1), [name]*(longbits+1))]\n",
    "            \n",
    "            else:\n",
    "                name_fp = [j+\"_\"+str(i) for i, j in zip(range(1,167+1), [name]*(167+1))]\n",
    "            hey = df.loc[index, name]\n",
    "            fps = {n: d for n, d in zip(name_fp, hey)}\n",
    "            for key in fps.keys():\n",
    "                df.loc[index, key] = fps[key]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quitar_fps(df):\n",
    "    fp_df = df.drop(fp_names, axis=1)\n",
    "    return fp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malaria",
   "language": "python",
   "name": "malaria"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
